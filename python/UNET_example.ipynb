{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Just an exercize\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torchvision.transforms.functional as TF\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DoubleConv(nn.Module):\r\n",
    "    def __init__(self, in_channels, out_channels):\r\n",
    "        super(DoubleConv, self).__init__()\r\n",
    "        self.conv = nn.Sequential(\r\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\r\n",
    "            nn.BatchNorm2d(out_channels),\r\n",
    "            nn.ReLU(inplace=True),\r\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\r\n",
    "            nn.BatchNorm2d(out_channels),\r\n",
    "            nn.ReLU(inplace=True)\r\n",
    "        )\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return self.conv(x)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class UNET(nn.Module):\r\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\r\n",
    "        super(UNET, self).__init__()\r\n",
    "        self.ups = nn.ModuleList()\r\n",
    "        self.downs = nn.ModuleList()\r\n",
    "        self.pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        for feature in features:\r\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\r\n",
    "            in_channels = feature\r\n",
    "\r\n",
    "        for feature in reversed(features):\r\n",
    "            self.ups.append(nn.ConvTranspose2d(\r\n",
    "                feature*2, feature, kernel_size=2, stride=2))\r\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\r\n",
    "\r\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\r\n",
    "        self.finalConv = nn.Conv2d(features[0], out_channels, kernel_size=1)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        skip_conn = []\r\n",
    "        for down in self.downs:\r\n",
    "            x = down(x)\r\n",
    "            skip_conn.append(x)\r\n",
    "            x = self.pool2d(x)\r\n",
    "\r\n",
    "        x = self.bottleneck(x)\r\n",
    "        skip_conn = skip_conn[::-1]\r\n",
    "\r\n",
    "        for idx in range(0, len(self.ups), 2):\r\n",
    "            x = self.ups[idx](x)\r\n",
    "            skip_co = skip_conn[idx//2]\r\n",
    "\r\n",
    "            if x.shape != skip_co.shape:\r\n",
    "                x = TF.resize(x, size=skip_co.shape[2:])\r\n",
    "\r\n",
    "            concat_skip = torch.cat((skip_co, x), dim=1)\r\n",
    "            x = self.ups[idx+1](concat_skip)\r\n",
    "\r\n",
    "        return self.finalConv(x)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = torch.randn((3, 1, 16, 16))\r\n",
    "model = UNET(in_channels=1, out_channels=1)\r\n",
    "preds = model(x)\r\n",
    "# print('prediction: {}'.format(preds))\r\n",
    "# print(\"original: {}\".format(x))\r\n",
    "\r\n",
    "assert preds.shape == x.shape\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "from PIL import Image\r\n",
    "from torch.utils.data import Dataset\r\n",
    "import numpy as np\r\n",
    "from dataset import KomatsDataset\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "class KomatsDataset(Dataset):\r\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\r\n",
    "        self.img_dir = img_dir\r\n",
    "        self.mask_dir = mask_dir\r\n",
    "        self.transform = transform\r\n",
    "        self.imgs = os.listdir(img_dir)\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.imgs)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        img_path = os.path.join(self.img_dir, self.imgs[index])\r\n",
    "        mask_path = os.path.join(self.mask_dir, self.imgs[index].replace(\"rgb\", \"label\"))\r\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\r\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\r\n",
    "        mask[mask != 0] = 1.0\r\n",
    "\r\n",
    "        if self.transform is not None:\r\n",
    "            augmentation = self.transform(image=image, mask=mask)\r\n",
    "            image = augmentation[\"image\"]\r\n",
    "            mask = augmentation[\"mask\"]\r\n",
    "\r\n",
    "\r\n",
    "        return image, mask"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import albumentations as A\r\n",
    "from albumentations.pytorch import ToTensorV2\r\n",
    "from tqdm import tqdm \r\n",
    "import torch.optim as optim \r\n",
    "from dataset import KomatsDataset\r\n",
    "from utils import (load_checkpoint, save_checkpoint, get_loaders, check_accuracy, save_predictions_as_imgs)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyper Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LEARNING_RATE = 1E-4\r\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "BATCH_SIZE = 3\r\n",
    "NUM_EPOCHS = 10\r\n",
    "NUM_WORKERS = 3\r\n",
    "IMAGE_HEIGHT = 480\r\n",
    "IMAGE_WIDTH = 480\r\n",
    "PIN_MEMORY = True\r\n",
    "LOAD_MODEL = True\r\n",
    "TRAIN_IMG_DIR = \"..\\dataset\\multi\\multi_plant\"\r\n",
    "TRAIN_IMG_LABEL = \"..\\dataset\\multi\\multi_label\"\r\n",
    "VAL_IMG_DIR = \"..\\dataset\\multi\\multi_plant_val\"\r\n",
    "VAL_IMG_LABEL = \"..\\dataset\\multi\\multi_label_val\"\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\r\n",
    "    loop = tqdm(loader)\r\n",
    "    for batch_idx, (data,  targets) in enumerate(loop):\r\n",
    "        data = data.to(device=DEVICE)\r\n",
    "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\r\n",
    "\r\n",
    "        with torch.cuda.amp.autocast():\r\n",
    "            predictions = model(data)\r\n",
    "            loss = loss_fn(predictions, targets)\r\n",
    "             \r\n",
    "        optimizer.zero_grad()\r\n",
    "        scaler.scale(loss).backward()\r\n",
    "        scaler.step(optimizer)\r\n",
    "        scaler.update()\r\n",
    "\r\n",
    "        loop.set_postfix(loss=loss.item())\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_transform = A.Compose(\r\n",
    "    [\r\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\r\n",
    "        A.Rotate(limit=35, p=1.0),\r\n",
    "        A.HorizontalFlip(p=0.5),\r\n",
    "        A.VerticalFlip(p=0.5),\r\n",
    "        A.Normalize(\r\n",
    "            mean=[0.0,0.0,0.0],\r\n",
    "            std=[1.0,1.0,1.0],\r\n",
    "            max_pixel_value=255.0\r\n",
    "        ),\r\n",
    "        ToTensorV2()\r\n",
    "    ]\r\n",
    "\r\n",
    ")\r\n",
    "val_transpose = A.Compose(\r\n",
    "    [\r\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\r\n",
    "        A.Normalize(\r\n",
    "            mean=[0.0, 0.0, 0.0],\r\n",
    "            std=[1.0, 1.0, 1.0],\r\n",
    "            max_pixel_value=255.0,\r\n",
    "        ),\r\n",
    "        ToTensorV2(),\r\n",
    "    ]\r\n",
    ")\r\n",
    "\r\n",
    "model = UNET(in_channels=3, out_channels=1).to(DEVICE)\r\n",
    "loss_fn = nn.BCEWithLogitsLoss()\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_loader, val_loader = get_loaders(\r\n",
    "    TRAIN_IMG_DIR,\r\n",
    "    TRAIN_IMG_LABEL,\r\n",
    "    VAL_IMG_DIR,\r\n",
    "    VAL_IMG_LABEL,\r\n",
    "    BATCH_SIZE,\r\n",
    "    train_transform,\r\n",
    "    val_transpose,\r\n",
    "    NUM_WORKERS,\r\n",
    "    PIN_MEMORY\r\n",
    ")\r\n",
    "\r\n",
    "scaler = torch.cuda.amp.GradScaler()\r\n",
    "\r\n",
    "for epoch in range(NUM_EPOCHS):\r\n",
    "    train_fn(train_loader, model, optimizer, loss_fn, scaler)\r\n",
    "    checkpoint = {\r\n",
    "        \"state_dict\": model.state_dict(),\r\n",
    "        \"optimizer\":optimizer.state_dict(),\r\n",
    "    }\r\n",
    "    save_checkpoint(checkpoint)\r\n",
    "\r\n",
    "    # check accuracy\r\n",
    "    check_accuracy(val_loader, model, device=DEVICE)\r\n",
    "\r\n",
    "    # print some examples to a folder\r\n",
    "    save_predictions_as_imgs(\r\n",
    "        val_loader, model, folder=\".\\saved_images\", device=DEVICE\r\n",
    "    )\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2381145111bc30c3424d071b9e1a3a4425b595547cfd3fc534ba4a3db9b7a9c"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('tesi': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}